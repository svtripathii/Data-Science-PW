{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d13fe7-3e4e-441d-8aef-2afb53d25289",
   "metadata": {},
   "source": [
    "# Data Science Masters Assignment - Feature Engineering 3\n",
    "\n",
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4faa5e5-b4bc-46d5-a3c2-fe850dc6ef70",
   "metadata": {},
   "source": [
    "Min-Max scaling is a normalization technique that scales the data to a fixed range, typically [0, 1] or [-1, 1]. \n",
    "This is useful when the scale of data varies widely, as scaling ensures each feature contributes proportionally to the model performance. \n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\\[\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}} \\times (max_{range} - min_{range}) + min_{range}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ea8bb-42a7-4551-9935-e2532b1f5e19",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let’s scale a dataset `[1, 5, 10, 15, 20]` to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cd2b7f-4748-4276-8096-b4ec22a027e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print('Scaled Data:', scaled_data.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54e847-2a82-4645-9957-1f1ef662e369",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf874e2-ad56-4ce7-93a6-fb2549130360",
   "metadata": {},
   "source": [
    "The Unit Vector technique (or normalization to unit norm) scales the data so that the norm (magnitude) of each data point is 1. \n",
    "This technique is more suited for applications where the direction of data points is more important than their absolute values, such as text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82744794-0d26-4f28-b2bc-58b5f9dcf644",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let’s apply Unit Vector scaling to the dataset `[1, 5, 10, 15, 20]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f9faaf-71b6-40ad-8511-22812bffbf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Vector Scaled Data: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalizing the data using unit vector technique\n",
    "unit_vector_scaled_data = normalize(data, norm='l2')\n",
    "print('Unit Vector Scaled Data:', unit_vector_scaled_data.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ceb9f-ec71-43ef-9fde-d66abf32fde8",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction?\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms the data into a set of orthogonal components called principal components. These components represent directions of maximum variance in the data.\n",
    "\n",
    "### Example\n",
    "We will reduce a 3D dataset to 2D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b0a3ed6-0878-4147-b15e-e675cc84051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data Shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creating a 3D dataset\n",
    "data_3d = np.random.randn(100, 3)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_2d = pca.fit_transform(data_3d)\n",
    "print('Transformed Data Shape:', data_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970c430-7b9f-4f24-963b-b1f5ee77f49c",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ae108-5da3-4ef9-bd46-3b45e29d4325",
   "metadata": {},
   "source": [
    "PCA can be used for Feature Extraction by transforming the original features into principal components. \n",
    "These components represent linear combinations of the original features that capture the most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc527db0-d56f-4b64-bd9c-48f4298805b2",
   "metadata": {},
   "source": [
    "## Q5. Min-Max scaling for a recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d593c50-41cd-4448-8a1a-6971140e9cca",
   "metadata": {},
   "source": [
    "In a food delivery recommendation system, Min-Max scaling can normalize features such as price, rating, and delivery time to a common range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71691ea4-38a9-499f-ab52-b2958a8676a8",
   "metadata": {},
   "source": [
    "### Example\n",
    "We will scale features for three orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c37608d1-6e4d-45c0-9e91-9a608fffb8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price    Rating  Delivery_Time\n",
      "0   -1.0  0.272727      -0.333333\n",
      "1    0.0 -1.000000      -1.000000\n",
      "2    1.0  1.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "recommendation_data = pd.DataFrame({\n",
    "    'Price': [10, 20, 30],\n",
    "    'Rating': [4.5, 3.8, 4.9],\n",
    "    'Delivery_Time': [30, 25, 40]\n",
    "})\n",
    "\n",
    "# Applying Min-Max scaling\n",
    "scaled_recommendation_data = scaler.fit_transform(recommendation_data)\n",
    "print(pd.DataFrame(scaled_recommendation_data, columns=recommendation_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea05af4-256c-4c32-8e20-22b2a3a3a1a9",
   "metadata": {},
   "source": [
    "## Q6. PCA for stock price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd23a44-7175-47c7-9ad1-9ca95e74740b",
   "metadata": {},
   "source": [
    "For stock price prediction, PCA can reduce the dimensionality of financial data by selecting principal components that capture the most variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370d903b-c4c5-4adc-8d7e-7fa2fc47f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Data Shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Simulated stock data with 10 features\n",
    "stock_data = np.random.randn(100, 10)\n",
    "\n",
    "# Applying PCA to reduce to 2 components\n",
    "pca_stock = PCA(n_components=2)\n",
    "stock_data_reduced = pca_stock.fit_transform(stock_data)\n",
    "print('Reduced Data Shape:', stock_data_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adecd25-fca5-48a6-93c0-6bd874b05e4f",
   "metadata": {},
   "source": [
    "## Q7. Min-Max scaling example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a163a-f171-48d6-9234-4078ed53998c",
   "metadata": {},
   "source": [
    "Min-Max scaling was already applied to `[1, 5, 10, 15, 20]` in Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d048b-c961-4c7d-a0e6-beb4258cd156",
   "metadata": {},
   "source": [
    "## Q8. PCA for feature extraction on a dataset\n",
    "\n",
    "Let’s apply PCA to a dataset with features `[height, weight, age, gender, blood pressure]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03987947-a595-4e8e-a7d6-07d2af3cc47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.26406698 0.22989685 0.1871897  0.18230549 0.13654098]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulating a dataset with 5 features\n",
    "feature_data = np.random.randn(100, 5)\n",
    "\n",
    "# Applying PCA\n",
    "pca_features = PCA()\n",
    "pca_features.fit(feature_data)\n",
    "explained_variance_ratio = pca_features.explained_variance_ratio_\n",
    "print('Explained Variance Ratio:', explained_variance_ratio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
